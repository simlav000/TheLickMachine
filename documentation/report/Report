\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
 \usepackage{hyperref}
 \usepackage{float}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{siunitx}
\begin{document}
\begin{titlepage}
    \centering
    {\Huge \textbf{The Lick Machine} \\ CNN Audio Pattern Recognition \par}
    \vspace{2.5cm}
    {\Huge Final Report \par}
    \vspace{2.5cm}
    {\Large Maddy Walkington (260 986 638) \\ Louis Bouchard (261 053 689) \\ Simon Pino-Buisson (261 051 516) \\ Simon Lavoie \par}
    \vspace{8.5cm}
    {\large December 13th, 2024 \par}
\end{titlepage}

\begin{abstract}
 
\end{abstract}

\section{Introduction}
\subsection{The Lick}
A popular inside-joke within jazz music circles centers around \textit{the lick}. The \textit{lick} is \href{https://www.youtube.com/watch?v=krDxhnaKD7Q&t=26s}{a short musical passage which is often played during improvisation}. The phenomenon began with a simple observation: "I've heard this passage before", and became a challenge: "How can I sneak \textit{the lick} into this solo?". Today, when \textit{the lick} is played during a set, the neurons of those in the know immediately fire, while going under the radar as a regular melody to those who aren't.

This project presents a convolutional neural network (hereby referred to as a CNN) trained to recognize this motif in a piece of music, making it also be part of this inside joke. For the model to succeed in recognizing \textit{the lick} in an audio sample, it would have to be invariant to the music's key, tempo, timbre, volume (see Appendix \ref{term}) and accompaniment of additional instruments.

\subsection{Music in Machine Learning}


\section{Literature Review}
\section{Methodology}
\subsection{Data Formatting}
As with all machine learning projects, training a CNN on such a complex task as audio recognition requires a large dataset. Finding a large supply of audio samples of \textit{the lick} didn't prove difficult; \href{https://www.kaggle.com/datasets/andychamberlain/the-lick/data}{a dataset of 80 thousand audio samples of \textit{the lick}} was easily found online. This pre-labelled dataset \cite{lickData} suits the needs of this project perfectly as half of the audio sample are \textit{the lick} with varying key, tempo, timbre, volume and accompaniments (which helps increase the model's adaptability), and the other half is made of various melodic segments that are not \textit{the lick} to help train the model to have falsifiable results.

Due to the lick being a relatively short motif, such an audio sample can be vectorized into a spectrogram which can be fed as input to a \textit{CNN}.


Apart from using the Pytorch library to create and train the model, the python library \href{https://librosa.org/doc/latest/index.html}{Librosa} \cite{Librosa} was used to extract the mel spectrogram of the audio datasets. A mel spectrogram is a visual representation of the frequency


\subsubsection{CNN Architecture}
\section{Results}
\section{Discussion}
\section{Conclusion}
\section{Contributions}

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\newpage
\appendix
\section{Music Terminology} \label{term}

\begin{itemize}
    \item \textbf{Key}: This is probably the most familiar term to those not familiar with music terminology. The key of a piece of music is its tonal center. The note that "sounds like home". Analogously to a coordinate system, given some arbitrary origin, changing the key for a melody would preserve the relative distances between its notes, while moving the ensemble to or away from the origin. The model must recognize \textit{the lick} by the relative distances between its notes, regardless of the key.

    \item \textbf{Tempo}: This is simply the speed at which the music is being played. Simply put, the model should recognize \textit{the lick} regardless of how fast or slow it is being played.

    \item \textbf{Timbre}: This term refers to the overall "sound quality" of a piece of music. This could encompass the notion of high-definition/poor quality when talking about compression of sound files, but is a more general term that encompasses how music sounds. Pianos produce a different sound than trombones do, even if they play the same note, at the same pitch, and the same loudness. In terms of sound data, timbre could be viewed as the fine-grain details within the waveform that aren't its amplitude or frequency. The model should recognize \textit{the lick} regardless of the instrument playing it.

    \item \textbf{Volume}: This is simply stating that our model should recognize \textit{the lick} regardless of how loud or quiet the lick is being played. In terms of the data, this means it must be invariant to waveform amplitude.
\end{itemize} 


\end{document}
